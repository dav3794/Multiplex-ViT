{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional, Tuple, Type, Dict, List\n",
    "\n",
    "from utils import window_partition, window_unpartition, patch_partition, patch_unpartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/naver-ai/rope-vit/blob/main/self-attn/rope_self_attn.py\n",
    "\n",
    "def init_2d_freqs(dim: int, num_heads: int, theta: float = 10.0, rotate: bool = True):\n",
    "    freqs_x = []\n",
    "    freqs_y = []\n",
    "    mag = 1 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "    for i in range(num_heads):\n",
    "        angles = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)        \n",
    "        fx = torch.cat([mag * torch.cos(angles), mag * torch.cos(torch.pi/2 + angles)], dim=-1)\n",
    "        fy = torch.cat([mag * torch.sin(angles), mag * torch.sin(torch.pi/2 + angles)], dim=-1)\n",
    "        freqs_x.append(fx)\n",
    "        freqs_y.append(fy)\n",
    "    freqs_x = torch.stack(freqs_x, dim=0)\n",
    "    freqs_y = torch.stack(freqs_y, dim=0)\n",
    "    freqs = torch.stack([freqs_x, freqs_y], dim=0)\n",
    "    return freqs\n",
    "\n",
    "def init_t_xy(end_x: int, end_y: int):\n",
    "    t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
    "    t_x = (t % end_x).float()\n",
    "    t_y = torch.div(t, end_x, rounding_mode='floor').float()\n",
    "    return t_x, t_y\n",
    "\n",
    "def compute_mixed_cis(freqs: torch.Tensor, t_x: torch.Tensor, t_y: torch.Tensor, num_heads: int):\n",
    "    N = t_x.shape[0]\n",
    "    # No float 16 for this range\n",
    "    # with torch.amp.autocast(freqs.device, enabled=False):\n",
    "    freqs_x = (t_x.unsqueeze(-1) @ freqs[0].unsqueeze(-2)).view(N, num_heads, -1).permute(1, 0, 2)\n",
    "    freqs_y = (t_y.unsqueeze(-1) @ freqs[1].unsqueeze(-2)).view(N, num_heads, -1).permute(1, 0, 2)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs_x), freqs_x + freqs_y)\n",
    "    return freqs_cis\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 1 < ndim\n",
    "    if freqs_cis.shape == (x.shape[-2], x.shape[-1]):\n",
    "        shape = [d if i >= ndim-2 else 1 for i, d in enumerate(x.shape)]\n",
    "\n",
    "    elif freqs_cis.shape == (x.shape[-3], x.shape[-2], x.shape[-1]):\n",
    "        shape = [d if i >= ndim-3 else 1 for i, d in enumerate(x.shape)]\n",
    "\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)\n",
    "\n",
    "def load_tokenizer(vocab_path: str = './tokenizer_vocab.json') -> Dict[str, int]:\n",
    "    \"\"\"Load tokenizer dictionary from its vocab json file.  \n",
    "\n",
    "    Args:\n",
    "        vocab_path (Optional[str]): Path to the vocab json file. Defaults to './tokenizer_vocab.json'.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: Dictionary mapping marker names to their respective indices.\n",
    "    \"\"\"\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        tokenizer = json.load(f)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Standard MLP module\"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            embedding_dim: int,\n",
    "            mlp_dim: int,\n",
    "            mlp_bias: bool = True,\n",
    "            act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dim, bias=mlp_bias),\n",
    "            act(),\n",
    "            nn.Linear(mlp_dim, embedding_dim, bias=mlp_bias),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "# code adapted from https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/image_encoder.py\n",
    "# and https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Multi-head Attention block with ROPE relative position embeddings (per channel/marker).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 768,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "        rope_theta: float = 10.,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n",
    "            rope_theta (float): Theta value for relative positional embeddings.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size. If None, use the default value of 16x16 and will be adjusted\n",
    "                to the input shape.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embedding_dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(embedding_dim, embedding_dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        self.compute_cis = partial(compute_mixed_cis, num_heads=self.num_heads)\n",
    "            \n",
    "        freqs = init_2d_freqs(\n",
    "            dim=head_dim, num_heads=self.num_heads, theta=rope_theta, \n",
    "            rotate=True\n",
    "        ).view(2, -1)\n",
    "        self.freqs = nn.Parameter(freqs, requires_grad=True)\n",
    "        \n",
    "        if input_size is not None:\n",
    "            end_x, end_y = input_size\n",
    "        else:\n",
    "            end_x, end_y = 16, 16\n",
    "\n",
    "        t_x, t_y = init_t_xy(end_x=end_x, end_y=end_y)\n",
    "        self.register_buffer('freqs_t_x', t_x)\n",
    "        self.register_buffer('freqs_t_y', t_y)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, _ = x.shape\n",
    "        # qkv with shape (3, B, nHead, H * W, C)\n",
    "        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # q, k, v with shape (B, nHead, H * W, C)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Apply rotary embeddings\n",
    "        t_x, t_y = self.freqs_t_x, self.freqs_t_y\n",
    "        \n",
    "        if self.freqs_t_x.shape[0] != H:\n",
    "            t_x, t_y = init_t_xy(end_x=W, end_y=H)\n",
    "            t_x, t_y = t_x.to(x.device), t_y.to(x.device)\n",
    "        freqs_cis = self.compute_cis(self.freqs, t_x, t_y)\n",
    "\n",
    "        q, k = apply_rotary_emb(q, k, freqs_cis=freqs_cis)        \n",
    "\n",
    "        # Attention\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialAttentionBlock(nn.Module):\n",
    "    \"\"\"Attention block with spatial (per-marker) attention on pixel level.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int = 8,\n",
    "        mlp_dim: int = 1024,\n",
    "        mlp_bias: bool = True,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        window_size: int = 0,\n",
    "        window_shift: Tuple[int, int] = (0, 0),\n",
    "        mini_batch: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            window_size (int): Window size for window attention blocks. If it equals 0, then\n",
    "                use global attention.\n",
    "            window_shift (Tuple[int, int]): Shift for window attention blocks.\n",
    "            mini_batch (int): Mini-batch size for window attention blocks. If it equals 0, then\n",
    "                all windows are processed at once.\n",
    "            \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(embedding_dim)\n",
    "        self.attn = SpatialAttention(\n",
    "            embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            input_size=(window_size, window_size) if window_size > 0 else None,\n",
    "        )\n",
    "\n",
    "        self.norm2 = norm_layer(embedding_dim)\n",
    "        self.mlp = MLP(embedding_dim=embedding_dim, mlp_dim=mlp_dim, mlp_bias=mlp_bias, act=act_layer)\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.window_shift = window_shift\n",
    "        self.mini_batch = mini_batch\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W, E = x.shape\n",
    "        x = x.reshape(B * C, H, W, E)\n",
    "        \n",
    "        # Window partition\n",
    "        if self.window_size > 0:\n",
    "            x, pad_hw = window_partition(x, self.window_size, self.window_shift) # [B * C * num_windows, window_size, window_size, E]\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        if self.mini_batch > 0:\n",
    "            # Iterate over slices of size of mini_batch\n",
    "            i, j = 0, self.mini_batch\n",
    "            while i < x.shape[0]:\n",
    "                x[i:j] = shortcut[i:j] + self.attn(x[i:j])\n",
    "                x[i:j] = x[i:j] + self.mlp(self.norm2(x[i:j]))\n",
    "                i, j = j, j + self.mini_batch\n",
    "        else:\n",
    "            x = self.attn(x)\n",
    "            x = shortcut + x\n",
    "            x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        # Reverse window partition\n",
    "        if self.window_size > 0:\n",
    "            x = window_unpartition(x, self.window_size, self.window_shift, pad_hw, (B, C, H, W, E))\n",
    "\n",
    "        x = x.reshape(B, C, H, W, E)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Multi-head Attention block (across channels/markers).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embedding_dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(embedding_dim, embedding_dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, _ = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, C, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4) # qkv with shape (3, B, nHead, C, E)\n",
    "        q, k, v = qkv.reshape(3, B * self.num_heads, C, -1).unbind(0) # q, k, v with shape (B * nHead, C, E)\n",
    "\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).view(B, self.num_heads, C, -1).permute(0, 2, 1, 3).reshape(B, C, -1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossChannelAttentionBlock(nn.Module):\n",
    "    \"\"\"Attention block with cross-channel (per-position) attention on patch level.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int = 8,\n",
    "        mlp_dim: int = 1024,\n",
    "        mlp_bias: bool = True,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        patch_size: int = 16,\n",
    "        patch_shift: Tuple[int, int] = (0, 0),\n",
    "        mini_batch: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            patch_size (int): Patch size for attention blocks.\n",
    "            patch_shift (Tuple[int, int]): Shift for attention blocks.\n",
    "            mini_batch (int): Mini-batch size for per-position patch blocks. If it equals 0, then\n",
    "                all patches are processed at once.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(embedding_dim)\n",
    "        self.attn = ChannelAttention(\n",
    "            embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "        )\n",
    "\n",
    "        self.norm2 = norm_layer(embedding_dim)\n",
    "        self.mlp = MLP(embedding_dim=embedding_dim, mlp_dim=mlp_dim, mlp_bias=mlp_bias, act=act_layer)\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        # self.patch_proj = nn.Parameter(torch.randn(patch_size * patch_size, embedding_dim))\n",
    "        self.patch_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.patch_shift = patch_shift\n",
    "\n",
    "        self.mini_batch = mini_batch\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W, E = x.shape\n",
    "\n",
    "        x, pad_hw = patch_partition(x, self.patch_size, self.patch_shift) # [B * num_patches, C, patch_size * patch_size, E]\n",
    "\n",
    "        # x_patched = torch.einsum(\"bcpe,pe->bce\", x, self.patch_proj) # [B * num_patches, C, E]\n",
    "        x_patched = self.patch_proj(x).mean(dim=2) # [B * num_patches, C, E]\n",
    "        x_patched = self.norm1(x_patched)\n",
    "\n",
    "        if self.mini_batch > 0:\n",
    "            # Iterate over slices of size of mini_batch\n",
    "            i, j = 0, self.mini_batch\n",
    "            while i < x_patched.shape[0]:\n",
    "                atn = self.attn(x_patched[i:j]).unsqueeze(2) # [mini_batch, C, 1, E]\n",
    "                atn = atn.expand_as(x[i:j]) # [mini_batch, C, patch_size * patch_size, E]\n",
    "\n",
    "                x[i:j] = x[i:j] + atn\n",
    "                x[i:j] = x[i:j] + self.norm2(self.mlp(x[i:j]))\n",
    "                i, j = j, j + self.mini_batch\n",
    "        else:\n",
    "            x = x + self.attn(x_patched).unsqueeze(2).expand_as(x)\n",
    "            x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        x = patch_unpartition(x, self.patch_size, self.patch_shift, pad_hw, (B, C, H, W, E))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class MultiplexBlock(nn.Module):\n",
    "    \"\"\"Multiplex Block integrating spatial and cross-channel attention blocks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        spatial_window_size: int,\n",
    "        channel_patch_size: int,\n",
    "        spatial_window_shift: Tuple[int, int] = (0, 0),\n",
    "        channel_patch_shift: Tuple[int, int] = (0, 0),\n",
    "        mlp_dim: int = 1024,\n",
    "        mlp_bias: bool = True,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        mini_batch: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads in each attention block.\n",
    "            spatial_window_size int: Window size for spatial attention blocks.\n",
    "            channel_patch_size int: Patch size for cross-channel attention blocks.\n",
    "            spatial_window_shift (Tuple[int, int]): Shift for spatial attention blocks.\n",
    "            channel_patch_shift (Tuple[int, int]): Shift for cross-channel attention blocks.\n",
    "            mlp_dim (int): Hidden dimension of the mlp.\n",
    "            mlp_bias (bool): If True, add a learnable bias to the mlp.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            mini_batch (int): Mini-batch size for patch and windows blocks. If it equals 0, then\n",
    "                all patches/windows are processed at once.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.spatial_attn = SpatialAttentionBlock(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            mlp_bias=mlp_bias,\n",
    "            qkv_bias=qkv_bias,\n",
    "            norm_layer=norm_layer,\n",
    "            act_layer=act_layer,\n",
    "            window_size=spatial_window_size,\n",
    "            window_shift=spatial_window_shift,\n",
    "            mini_batch=mini_batch\n",
    "        )\n",
    "\n",
    "        self.channel_attn = CrossChannelAttentionBlock(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            mlp_bias=mlp_bias,\n",
    "            qkv_bias=qkv_bias,\n",
    "            norm_layer=norm_layer,\n",
    "            act_layer=act_layer,\n",
    "            patch_size=channel_patch_size,\n",
    "            patch_shift=channel_patch_shift,\n",
    "            mini_batch=mini_batch\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.spatial_attn(x)\n",
    "        x = self.channel_attn(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "class MultiplexImageTransformer(nn.Module):\n",
    "    \"\"\"Multiplex Block integrating spatial and cross-channel attention blocks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        spatial_window_sizes: List[int],\n",
    "        channel_patch_sizes: List[int],\n",
    "        shift_sec: bool = True,\n",
    "        mlp_dim: int = 1024,\n",
    "        mlp_bias: bool = True,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        mini_batch: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            num_layers (int): Number of attention blocks.\n",
    "            spatial_window_sizes (List[int]): Window sizes for spatial attention blocks.\n",
    "            channel_patch_sizes (List[int]): Patch sizes for cross-channel attention blocks.\n",
    "            shift_sec (bool): If True, shift patches/windows by half of their size every second layer.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            mini_batch (int): Mini-batch size for patch and windows blocks. If it equals 0, then\n",
    "                all patches/windows are processed at once.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        assert len(spatial_window_sizes) == num_layers, \"Number of spatial window sizes must match number of layers.\"\n",
    "        assert len(channel_patch_sizes) == num_layers, \"Number of channel patch sizes must match number of layers.\"\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(\n",
    "                MultiplexBlock(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    spatial_window_size=spatial_window_sizes[i],\n",
    "                    channel_patch_size=channel_patch_sizes[i],\n",
    "                    spatial_window_shift=(spatial_window_sizes[i]//2, spatial_window_sizes[i]//2) if shift_sec and i % 2 == 1 else (0, 0),\n",
    "                    channel_patch_shift=(channel_patch_sizes[i]//2, channel_patch_sizes[i]//2) if shift_sec and i % 2 == 1 else (0, 0),\n",
    "                    mlp_dim=mlp_dim,\n",
    "                    mlp_bias=mlp_bias,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    norm_layer=norm_layer,\n",
    "                    act_layer=act_layer,\n",
    "                    mini_batch=mini_batch\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atn = SpatialAttention(768, 8)\n",
    "# input = torch.rand(3, 16, 16, 768)\n",
    "# output = atn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blck = SpatialAttentionBlock(embedding_dim=512, window_size=16, mini_batch=10, window_shift=(8,8)).to(device)\n",
    "input = torch.rand(1, 40, 100, 100, 512).to(device)\n",
    "output = blck(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blck = CrossChannelAttentionBlock(embedding_dim=512, patch_size=16, mini_batch=0, patch_shift=(8,8)).to(device)\n",
    "input = torch.rand(1, 40, 100, 100, 512).to(device)\n",
    "output = blck(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blck = MultiplexBlock(embedding_dim=512, num_heads=8, spatial_window_size=8, channel_patch_size=8, mini_batch=32).to(device)\n",
    "input = torch.rand(1, 40, 100, 100, 512).to(device)\n",
    "output = blck(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 31.62 GB, other allocations: 4.65 GB, max allowed: 36.27 GB). Tried to allocate 128.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m, embedding_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# with autocast(device_type=device, dtype=torch.float16):\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 413\u001b[0m, in \u001b[0;36mMultiplexImageTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 413\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 351\u001b[0m, in \u001b[0;36mMultiplexBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 351\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspatial_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_attn(x)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 158\u001b[0m, in \u001b[0;36mSpatialAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    156\u001b[0m i, j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmini_batch\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 158\u001b[0m     x[i:j] \u001b[38;5;241m=\u001b[39m shortcut[i:j] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     x[i:j] \u001b[38;5;241m=\u001b[39m x[i:j] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x[i:j]))\n\u001b[1;32m    160\u001b[0m     i, j \u001b[38;5;241m=\u001b[39m j, j \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmini_batch\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m, in \u001b[0;36mSpatialAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m q, k \u001b[38;5;241m=\u001b[39m apply_rotary_emb(q, k, freqs_cis\u001b[38;5;241m=\u001b[39mfreqs_cis)        \n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Attention\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 31.62 GB, other allocations: 4.65 GB, max allowed: 36.27 GB). Tried to allocate 128.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "model = MultiplexImageTransformer(\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    mlp_dim=embedding_dim*2,\n",
    "    spatial_window_sizes=[8, 8, 16],\n",
    "    channel_patch_sizes=[1, 1, 4],\n",
    "    mini_batch=1\n",
    ").to(device)\n",
    "\n",
    "\n",
    "input = torch.rand(1, 40, 112, 112, embedding_dim).to(device)\n",
    "# with autocast(device_type=device, dtype=torch.float16):\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
