{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast\n",
    "\n",
    "# from functools import partial\n",
    "from typing import Optional, Tuple, Type, Dict, List\n",
    "\n",
    "# from utils import window_partition, window_unpartition, patch_partition, patch_unpartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalResponseNormalization(nn.Module):\n",
    "    \"\"\"Global Response Normalization (GRN) layer \n",
    "    from https://arxiv.org/pdf/2301.00808\"\"\"\n",
    "\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # B, C, H, W, E = x.shape\n",
    "\n",
    "        gx = torch.norm(x, p=2, dim=(2,3), keepdim=True)\n",
    "        nx = gx / (gx.mean(dim=-1, keepdim=True) + self.eps)\n",
    "        return self.gamma * (x * nx) + self.beta + x\n",
    "\n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    \"\"\"ConvNext2 block\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            inter_dim: int,\n",
    "    ):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "            self.ln = nn.LayerNorm(dim)\n",
    "            self.conv2 = nn.Linear(dim, inter_dim) # equivalent to nn.Conv2d(dim, inter_dim, kernel_size=1)\n",
    "            self.act = nn.GELU()\n",
    "            self.grn = GlobalResponseNormalization()\n",
    "            self.conv3 = nn.Linear(inter_dim, dim) # equivalent to nn.Conv2d(inter_dim, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # B, H, W, E = x.shape\n",
    "        residual = x\n",
    "        x = x.permute(0, 3, 1, 2) # [B, E, H, W]\n",
    "        x = self.conv1(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, H, W, E]\n",
    "        x = self.ln(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x + residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Standard MLP module\"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            embedding_dim: int,\n",
    "            mlp_dim: int,\n",
    "            mlp_bias: bool = True,\n",
    "            act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dim, bias=mlp_bias),\n",
    "            act(),\n",
    "            nn.Linear(mlp_dim, embedding_dim, bias=mlp_bias),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class SpatioChannelAttention(nn.Module):\n",
    "    \"\"\"Spatial convolutional blocks (per channel/marker)\n",
    "    and cross-channel attention on pixel level.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        num_conv_blocks: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            num_conv_blocks (int): Number of spatial convolutional blocks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert embedding_dim % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
    "        head_dim = embedding_dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.spatial_conv_blocks = nn.ModuleList()\n",
    "        for _ in range(num_conv_blocks):\n",
    "            self.spatial_conv_blocks.append(\n",
    "                ConvNextBlock(\n",
    "                    dim=embedding_dim,\n",
    "                    inter_dim=4*embedding_dim,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.qkv = nn.Linear(embedding_dim, 3*embedding_dim)\n",
    "\n",
    "        self.proj = MLP(embedding_dim=embedding_dim, mlp_dim=2*embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W, E = x.shape\n",
    "        N_HEADS = self.num_heads\n",
    "        shortcut = x\n",
    "\n",
    "        # flatten on channel and batch dimension\n",
    "        x = x.reshape(B * C, H, W, E) # [B * C, H, W, E]\n",
    "\n",
    "        # Spatial convolutional blocks\n",
    "        for block in self.spatial_conv_blocks:\n",
    "            x = block(x)\n",
    "        residual = x\n",
    "\n",
    "        # Cross-channel attention\n",
    "        x = x.reshape(B, C, H, W, E).permute(0, 2, 3, 1, 4).reshape(-1, C, E) # [B*H*W, C, E]\n",
    "        qkv = self.qkv(x).reshape(B * H * W, C, 3, N_HEADS, -1).permute(2, 0, 3, 1, 4) # [3, B * H * W, C, num_heads, E/num_heads]\n",
    "        q, k, v = qkv.unbind(0) # [B * H * W, C, num_heads, E/num_heads]\n",
    "\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).reshape(B, H, W, C, N_HEADS, -1).permute(0, 3, 1, 2, 4, 5).reshape(B, C, H, W, E)\n",
    "        x = x + residual\n",
    "        x = self.proj(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiplexBlock(nn.Module):\n",
    "    \"\"\"Multiplex Block integrating spatio-channel attention blocks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_blocks (int): Number of attention blocks.\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads in each attention block.  \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            self.blocks.append(\n",
    "                SpatioChannelAttention(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "class MultiplexImageTransformer(nn.Module):\n",
    "    \"\"\"Multiplex Image Transformer integrating spatial and cross-channel blocks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_blocks: List[int],\n",
    "        embedding_dims: List[int],\n",
    "        num_heads: List[int],\n",
    "        channel_embedding_dim: int,\n",
    "        num_channels: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layers_blocks (List[int]): Number of attention blocks in each layer.\n",
    "            embedding_dims (List[int]): Number of input channels in each layer.\n",
    "            num_heads (List[int]): Number of attention heads in each attention block.\n",
    "            channel_embedding_dim (int): Embedding dimension per channel pixel.\n",
    "            num_channels (int): Maximal number of channels/markers (vocab dim).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channel_embedder = nn.Embedding(num_channels, channel_embedding_dim)\n",
    "\n",
    "        # self.stem = nn.Conv2d(channel_embedding_dim, embedding_dims[0], kernel_size=4, padding=0, stride=4)\n",
    "        self.poolings = nn.ModuleList()\n",
    "        self.poolings.append(\n",
    "            nn.Conv2d(channel_embedding_dim, embedding_dims[0], kernel_size=4, padding=0, stride=4)\n",
    "        )\n",
    "        for i, out_dim in enumerate(embedding_dims[1:]):\n",
    "            input_dim = embedding_dims[i]\n",
    "            self.poolings.append(\n",
    "                nn.Conv2d(input_dim, out_dim, kernel_size=2, padding=0, stride=2)\n",
    "            )\n",
    "             \n",
    "        self.layers = nn.ModuleList()\n",
    "        for blocks, dim, heads in zip(layers_blocks, embedding_dims, num_heads):\n",
    "            self.layers.append(\n",
    "                MultiplexBlock(\n",
    "                    num_blocks=blocks,\n",
    "                    embedding_dim=dim,\n",
    "                    num_heads=heads,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, channel_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Multiplex Image Transformer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Multiplex images batch tensor with shape [B, C, H, W]\n",
    "            channel_ids (torch.Tensor): Channel ids tensor with shape [B, C]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embedding tensor\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        E = self.channel_embedder.embedding_dim\n",
    "\n",
    "        channel_embeds = self.channel_embedder(channel_ids)\n",
    "        x = x.unsqueeze(-1)  # [B, C, H, W, 1]\n",
    "        channel_embeds = channel_embeds.unsqueeze(-2).unsqueeze(-2) # [B, C, 1, 1, E]\n",
    "\n",
    "        # channel embedding\n",
    "        x = x * channel_embeds # [B, C, H, W, E]\n",
    "\n",
    "        # poolings and blocks\n",
    "        for pooling, layer in zip(self.poolings, self.layers):\n",
    "            x = x.reshape(B * C, H, W, E).permute(0, 3, 1, 2)\n",
    "            x = pooling(x)\n",
    "            _, E, H, W = x.shape\n",
    "            x = x.permute(0, 2, 3, 1).reshape(B, C, H, W, E)\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blck = SpatioChannelAttention(embedding_dim=96, num_heads=1, num_conv_blocks=3).to(device)\n",
    "input = torch.rand(1, 40, 128, 128, 96).to(device)\n",
    "output = blck(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blck = MultiplexBlock(num_blocks=1, embedding_dim=96, num_heads=1).to(device)\n",
    "input = torch.rand(1, 40, 128, 128, 96).to(device)\n",
    "output = blck(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blck = MultiplexBlock(embedding_dim=512, num_heads=8, spatial_window_size=8, channel_patch_size=8, mini_batch=32).to(device)\n",
    "input = torch.rand(1, 40, 128, 128, 96).to(device)\n",
    "output = blck(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_blocks = [3, 3, 3, 3]\n",
    "embedding_dims = [96, 192, 384, 768]\n",
    "num_heads = [1, 2, 4, 8]\n",
    "\n",
    "model = MultiplexImageTransformer(\n",
    "    layers_blocks=layers_blocks,\n",
    "    embedding_dims=embedding_dims,\n",
    "    num_heads=num_heads,\n",
    "    channel_embedding_dim=48,\n",
    "    num_channels=40,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "input = torch.rand(1, 40, 224, 224).to(device)\n",
    "channel_ids = torch.randint(0, 40, (1, 40)).to(device)\n",
    "# with autocast(device_type=device, dtype=torch.float16):\n",
    "output = model(input, channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
